% !TeX root = ../thuthesis-example.tex

\chapter{总结与展望}

\section{研究工作总结}

本文通过理论假设和实验验证的方法针对卷积神经网络如何遗忘的问题进行了系统研究。本文正文共有五个章节，分别是引言、相关研究工作、卷积神经网络的分层抽象特性与遗忘方法、遗忘方法的实现和验证以及总结和展望。

第一章引言中首先介绍了遗忘问题产生的背景与研究的意义。随着卷积神经网络的广泛应用，越来越多的数据被用于卷积神经网络的训练中。然而，随着在线机器学习服务的兴起，带来的问题愈加明显。
有一些研究者发现通过仅仅通过神经网络的输出数据就能对神经网络模型的输入数据进行还原，或者对神经网络模型训练出来的参数进行抽取，或者能判断出某个输入是否曾经被用于训练该模型。
这些攻击手段的出现给我们发出的一个明显信号就是神经网络模型中包含重要的信息，很可能会出现数据泄露甚至个人隐私数据泄漏的问题。这是从实际出发提出来的数据需要遗忘的需求。
从法律角度上看，机器学习模型也是需要遗忘的。欧盟出台的《通用数据保护条例（GDPR）》从法律层面上规定了用户拥有“被遗忘权”。当用户需要删除数据时，数据管理者要及时履行约定。
基于这些事实，我们研究卷积神经网络的遗忘具有重要的现实意义。
接着，第一章介绍了卷积神经网络遗忘技术的最新发展现状。经过搜集和整理后发现，目前针对卷积神经网络的遗忘问题研究者们提出了多种解决方案，例如基于牛顿梯度更新的方法和基于网络分割的方法等，但是还没有一个成熟稳定的理论。
随后，第一章介绍了本文设计遗忘方法的主要思想和实验验证的效果。在第一章最后介绍了本文主要的写作框架。

在第二章中，本文主要介绍了和卷积神经网络研究方向相关的具有代表性的研究工作，具体的方向涉及到迁移学习，增量学习还有遗忘学习。
迁移学习的思想和本文的思路在某种程度上是契合的，目的都是无需重新训练整个模型就能将快速地实现模型知识的迁移，对本文具有一定的启发意义。
增量学习技术的出发点是克服神经网络本身固有的特点，即灾难性遗忘。随着新的训练数据继续在网络中训练，原先已经训练过的类别在准确率上有不同程度的降低，即发生了灾难性遗忘的现象。
为了解决这个问题，研究人员尝试了多种在无需重新训练全部模型的情况下，想办法将新的类别加入到原有的网络模型当中。其中有一类方法就是共享网络参数的方法，就是将低层次的网络参数作为共享参数，然后不断地在靠近输出的层次上面增加神经网络，同时冻结共享参数。
然而，这样的方法对于增量学习来说具有一定的挑战性。因为新类别的数据可能具有自己独特的基本特征，如果冻结较为低层次的网络参数，可能会导致无法学习新类别的这些基本特征信息。
但是对于神经网络的遗忘就无需考虑这个问题，因为类别是在减少的，不用担心会有新的基本特征需要学习。
在遗忘学习的研究方面，一些研究人员已经尝试使用牛顿更新的方法，试图将消除待遗忘的训练数据带来的影响，可是效果不是很理想。
另外一些相关工作试图使用差分隐私的思路，给网络中的参数增加噪音，从而达到遗忘的目的。然而这样的方法会给保留的类别带来准确率下降的问题。总之，在卷积神经网络的遗忘方面尚没有成熟的理论。

第三章是本文的核心章节，在第三章为了让读者更好地理解卷积神经网络的分层抽象特性，我们首先介绍了卷积神经网络基本结构和原理。
接着介绍了卷积神经网络的分层抽象特性。就是卷积神经网络在较低层次提取的是输入的一些基本信息，就好像人类视觉中的具有差异性的感受野，有的感受野对横向移动感兴趣，有的感受也对纵向移动感兴趣。
随着卷积网络层次的提高，卷积核中提取的特征越来越抽象，感受野也越来越大，从而提取到特征的范围也越来越大。靠近输出层的卷积层提取到的是和分类相关的特征。
从这个特性我们受到了一定的启发，能不能不用更新所有的网络参数，只更新和分类关联比较大的网络参数。带着这个思路我们设计了一套遗忘方法，首先确定分层的数值。
为了确定这个数值，我们综合考虑了三个指标的数值。确定好分层后，接下来就要重置网络参数，我们首先将网络训练的初始参数保留下来，然后待需要重置参数时，将初始参数复制到相对应的层次上，这就完成了参数的重置过程。
再使用保留集对网络进行冻结部分参数的训练，被冻结的参数是没有被重置的参数，这么做是为了共享之前已经学习好的基本特征。
训练至网络收敛之后就完成了卷积神经网络的遗忘过程。我们为了测试遗忘的效果，选取了三个不同方面的测试指标，分别是测试集准确率，收敛时间还有激活距离。

第四章是对第三章遗忘方法的实验验证。我们设计了四个实验来进行验证，分别是确定冻结层数实验，冻结必要性验证实验，反向冻结验证实验和遗忘可持续性验证实验。确定冻结层数实验，目的是确定冻结参数的层次。
实验结果表明，对于Resnet18的网络结构来说，重置倒数第6层是个很好的选择。
冻结必要性实验的目的是对比更新网络参数后是否有必要对网络进行冻结。实验中设计了一套对比实验，通过三个指标的对比表明，冻结参数对提高遗忘效果是有必要的。
反向冻结实验作为本文提到的遗忘方法的一个对照实验，目的是验证分层抽象特性的有效性。从实验结果来看，分层抽象特性的效果是很明显的。
最后一个实验是遗忘可持续性验证实验，这个实验旨在探究本文提到的遗忘方法是否能够用来连续进行遗忘操作。最终实验结果表明，无论需要遗忘多少类别，本方法是均可以达到理想的遗忘效果。

\section{存在问题与展望}
随着卷积神经网络的应用越来越多，会有越来越多的数据被用来训练网络。GDPR法规的出台，神经网络不仅要研究如何训练，也要注意如何才能使得学习到的信息不能被攻击者窃取。
用户的隐私意识也在逐步地增强，相信会有越来越多的用户会提出要数据控制者删除数据的需求。本文虽然在一定的范围内取得了很好的效果，但是仍有以下几点不足。

第一，本方法只能针对保留数据集的情况下使用。有一些实际情况是训练数据不是实时可用，这就导致重置网络参数后无法恢复保留类别原有的准确率。如何不用保留数据集就能实现很好的遗忘效果是一个值得后续研究的方向。

第二，本方法只适用于遗忘整个类别的情况，对于遗忘单个数据的操作，本方法不能做到很好的支持。作者认为有一些数据一旦被用于学习以后，其影响是无法消除的，因此无法被遗忘，有一些相关工作\cite{2018arXiv181205159T}的结论也支持这一点。

\section{本章小结}
本章对全文进行了概括性的总结并对本文目前尚未做到的工作进行了说明，同时又进行了一定的展望。随着卷积神经网络的普遍应用，用户隐私越来越受到重视，基于卷积神经网络的遗忘方法的相关研究势在必行。