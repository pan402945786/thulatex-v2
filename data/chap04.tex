% !TeX root = ../thuthesis-example.tex

\chapter{遗忘方法实现与验证}

\section{实验介绍}

\subsection{实验环境介绍}
我们用到的实验设备是两台服务器，如表\ref{tab:experiment-deivce}所示。
\begin{table}
    \centering
    \caption{实验设备情况}
    \begin{tabular}{lll}
      \toprule
      属性  & 设备1 & 设备2  \\
      \midrule
      CPU   & Core(TM) i9-9900K CPU @ 3.60GHz & Core(TM) i7-6700K CPU @ 4.00GHz \\
      内存大小  & 32G & 32G                    \\
      硬盘类型 & SSD  & SSD  \\
      显卡类型 & Nvidia Geforce 2080  & Nvidia Geforce 1080  \\
      显卡数量 & 1  & 3  \\
      操作系统 & Ubuntu 16.04LTS  & Ubuntu 16.04LTS  \\
      Python版本 & 3.8.7  & 3.8.8  \\
      Pytorch版本 & 1.7.1  & 1.8.0  \\
      显卡驱动版本 & 460.56  & 455.23.05  \\
      CUDA版本 & 11.2  & 11.1  \\
      \bottomrule
    \end{tabular}
    \label{tab:experiment-deivce}
\end{table}

我们使用的深度学习框架是Pytorch，数据集是CIFAR10\cite{cifar10_2009}，正常训练集有50000张图片，测试集有10000张图片。这个数据集共有10个类别，每个类别有5000张训练数据和1000张测试数据。
在确定冻结层数实验中，我们默认遗忘两个类别。因此遗忘训练集是从正常训练集分离出来的10000张图片，遗忘测试集2000张图片。保留训练集是指正常训练集除去了遗忘集以外的数据集合。
保留训练集40000张图片，保留测试集8000张图片。我们使用的神经网络框架是ResNet18\cite{He_2016_CVPR}，是一个具有代表性的卷积神经网络。
为了对比实验效果，我们在确定冻结参数层次实验中也使用了VGG16\cite{Simonyan15}的网络结构进行了实验。实验中我们对VGG16网络结构的最后两层进行了删除，因此实际只有14个层次，其中包含13个卷积层和1个全连接层。

在训练时我们使用了自适应学习率的方法，自适应的策略是如果该轮训练后测试集的准确率连续3次低于历史最好值，则学习率减半。每次训练数量（BATCH\_SIZE）是100，初始学习率（LEARNING\_RATE）为0.1。训练的方法使用随机梯度下降(SGD)训练方法，权重衰减参数（WEIGHT\_DECAY）为0.0005，冲量参数（MOMENTUM）为0.9。

\subsection{实验设计}
\subsubsection{确定冻结层数实验}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.25]{chapter4_process_1.png}
%     \caption{确定冻结层数实验流程图}
%     \label{fig:chapter4_process_1}
% \end{figure}

确定冻结层数的方法在前面有所介绍，这章我们在ResNet18和改造过的VGG16神经网络上进行确定冻结层数的实验，以检验我们提出方法的有效性。
我们首先在ResNet18网络结构上用CIFAR10数据集进行了确定层数实验。首先是要获得预选的层数，为此我们将参数进行反向冻结，然后用保留集训练模型至收敛。
我们以反向重置11层参数为例，如图\ref{fig:chapter4_resnet_reverse_1}所示。图中将靠近输入的前11层参数进行了初始化，对后7层的参数保留其原来参数。
在训练过程中，只更新前11层的参数，保持后7层参数不更新。像上面的例子一样，从靠近输入层的参数开始，逐层加入到训练参数的部分，然后保持其余参数不变，并且在训练过程中不被更新。
完成上述过程之后，就会得到和网络层数相同数量的网络模型。依次测试每个模型的遗忘集准确率，找到“第一个零点”就能得到预选层数。
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{chapter4_resnet_reverse_1.png}
    \caption{ResNet18反向冻结参数示意图}
    \label{fig:chapter4_resnet_reverse_1}
\end{figure}

得到预选层数之后，我们通过正向冻结参数的方法进行分层评估。具体方法是：
首先将参数分成前面部分和后面部分。其中前面部分是指离输入层比较近的参数，后面部分是指离输出层比较近的参数。前面部分的参数保留原来的参数，并且在训练过程中不更新参数。
后面部分的参数在训练之前进行初始化操作，并且在训练过程中更新参数。从离输出层最近的一层开始，将最后一层参数加入到后面部分，其余参数加入到前面参数。使用保留数据训练网络至收敛后便得到一个模型。
再将离输出层最近的两层加入到后面部分，其余参数加入到前面部分，然后再使用保留集训练至网络收敛。以此类推，直到所有网络参数全部加入到后面部分，前面部分没有参数为止。就能得到与网络层数个数相同的模型。
正向冻结参数的方法如图\ref{fig:chapter4_resnet_normal_1}所示，图中展示了前面部分有11层参数，后面部分有7层参数。
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{chapter4_resnet_normal_1.png}
    \caption{ResNet18正向冻结参数示意图}
    \label{fig:chapter4_resnet_normal_1}
\end{figure}
得到模型后，使用保留测试集和遗忘测试集测试各个网络模型，能够得到相应的准确率。
在训练过程中，记录下每个Epoch的平均损失函数值，通过这些损失函数值就能够得出模型收敛至一定数值时训练的批次数，从而能够评估预选层数及预选层数周边层数的遗忘效果和遗忘效率。

本实验使用的遗忘测试集包含CIFAR10中轮船和卡车的测试数据，保留训练集包含CIFAR10中除了轮船和卡车以外所有的训练数据，保留测试集包含除了轮船和卡车以外所有的测试数据。
保留训练集、保留测试集、遗忘训练集和遗忘测试集的生成方法如算法\ref{algorithm:dispartDataset}所示。表\ref{tab:forget-continuous-2-kinds}中展示了遗忘类别与保留类别的信息。
\begin{algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{分离数据集方法  dispartDataset}
	\label{algorithm:dispartDataset}
	\begin{algorithmic}[1]
        \REQUIRE 训练集$trainset$,测试集$testset$,遗忘类别$C_{forget}$
        \ENSURE  $trainRetain,trainForget,testRetain,testForget$
        \FOR {$i \to trainset.length$}
            \IF{$trainset[i].label \in C_{forget}$}
                \STATE $ trainRetain.push(trainset[i])$
            \ELSE
                \STATE $ trainForget.push(trainset[i])$
            \ENDIF
            \IF{$testset[i].label \in C_{forget}$}
                \STATE $ testRetain.push(testset[i])$
            \ELSE
                \STATE $ testForget.push(testset[i])$
            \ENDIF
        \ENDFOR
        \RETURN $trainRetain,trainForget,testRetain,testForget$
	\end{algorithmic}  
\end{algorithm}
\begin{table}
    \centering
    \caption{确定冻结层数实验遗忘类别和保留类别}
    \begin{tabular}{cp{3cm}p{7cm}p{7cm}}
      \toprule 
      遗忘类别数  & 遗忘类别 & 保留类别  \\
      \midrule
      2 & 轮船、卡车  & 飞机、汽车、鸟、猫、鹿、狗、青蛙、马  \\
      \bottomrule
    \end{tabular}
    \label{tab:forget-continuous-2-kinds}
\end{table}
我们也使用了VGG16网络结构进行了确定层数实验，实验过程同ResNet18网络结构的实验过程基本相同，这里就不再赘述。

\subsubsection{冻结必要性验证实验}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.25]{chapter4_process_2.png}
%     \caption{冻结必要性实验流程图}
%     \label{fig:chapter4_process_2}
% \end{figure}

通过本实验可以验证冻结参数能否加快遗忘训练收敛速度。
本实验的实验过程和确定冻结层数实验大致相同。不同的地方在于训练模型至收敛的过程中无需冻结参数，以对比不冻结参数对遗忘效果的影响程度。
如图\ref{fig:chapter4_resnet_no_freezing_1}所示，图中展示了ResNet18网络上进行不冻结参数实验的过程。
我们首先将ResNet18网络模型的参数分成两个部分，前面部分和后面部分。前面部分是靠近输入的部分，后面部分是靠近输出的部分。
分层方法依然是遍历所有层次。首先将靠近输出层的最近一层参数作为后面部分，其余参数作为前面部分。经过一轮实验后再将靠近输出层的两层参数作为后面部分，其余参数作为前面部分，再进行一轮实验。
以此类推，直到所有参数均作为后面部分，没有前面参数为止。
在分好前面部分和后面部分后，我们开始处理参数。
在每一轮实验前，将后面部分的参数进行初始化，前面部分的参数保持原来的状态。此时参数处理已经完成。
然后使用保留训练集训练处理好参数的网络，网络中的前面部分和后面部分均不冻结参数，直到网络训练收敛为止。
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{chapter4_resnet_no_freezing_1.png}
    \caption{ResNet18不冻结参数训练示意图}
    \label{fig:chapter4_resnet_no_freezing_1}
\end{figure}

具体方法如算法\ref{algorithm:noFreezingResetMain}所示。
\begin{algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{冻结必要性验证实验算法 noFreezingResetMain}
	\label{algorithm:noFreezingResetMain}
	\begin{algorithmic}[1]
        \REQUIRE 模型总层数$totalLayers$,遗忘类别$C_{forget}$,全部测试集$D_{all\_train}$,全部测试集$D_{all\_test}$,初始网络模型$net$
        \ENSURE  $accNormalFreezeRetainArr,accNormalFreezeForgetArr,$\\$distanceNormalFreezeRetainArr,distanceNormalFreezeForgetArr,T_{converge\_i}Arr$
        \STATE $accNormalFreezeRetainArr,accNormalFreezeForgetArr,$\\$distanceNormalFreezeRetainArr,distanceNormalFreezeForgetArr,T_{converge\_i}Arr = []$
        \STATE $D_{retain\_train}, D_{forget\_train}, D_{retain\_test}, D_{forget\_test} \gets dispartDataset(D_{all\_train}, D_{all\_test}, C_{forget})$(算法\ref{algorithm:dispartDataset})
        \STATE $modelInit \gets net.saveModel()$
        \STATE $modelNormal,\_ \gets trainNet(modelInit, trainset, P_{freeze}, T_{threshold})$(算法\ref{algorithm:trainNet})
        \STATE $modelRetrain,\_ \gets trainNet(modelInit, D_{retain\_train}, [], T_{threshold})$(算法\ref{algorithm:trainNet})
        \FOR {$i \to totalLayers$}
            \STATE $modelResetBeforeTrain\_i \gets mergeModel(modelNormal, modelInit, totalLayers, i)$(算法\ref{algorithm:mergeModel})
            \STATE $modelResetAfterTrain\_i,T_{converge\_i} \gets trainNet(modelResetBeforeTrain\_i , D_{retain\_train},$\\$ [], T_{threshold})$(算法\ref{algorithm:trainNet})
            \STATE $accNormalFreezeRetain\_i \gets getAcc(modelResetAfterTrain\_i, D_{retain\_test})$(算法\ref{algorithm:getAcc})
            \STATE $accNormalFreezeForget\_i  \gets getAcc(modelResetAfterTrain\_i, D_{forget\_test})$(算法\ref{algorithm:getAcc})
            \STATE $distanceNormalFreezeRetain\_i \gets getDistance(model, modelCmp, D_{retain\_test})$(算法\ref{algorithm:getDistance})
            \STATE $distanceNormalFreezeForget\_i \gets getDistance(model, modelCmp, D_{forget\_test})$(算法\ref{algorithm:getDistance})
            \STATE $accNormalFreezeRetainArr.push(accNormalFreezeRetain\_i)$
            \STATE $accNormalFreezeForgetArr.push(accNormalFreezeForget\_i)$
            \STATE $distanceNormalFreezeRetainArr.push(distanceNormalFreezeRetain\_i)$
            \STATE $distanceNormalFreezeForgetArr.push(distanceNormalFreezeForget\_i)$
            \STATE $T_{converge\_i}Arr.push(T_{converge\_i})$
        \ENDFOR
        \RETURN $accNormalFreezeRetainArr,accNormalFreezeForgetArr,$\\$distanceNormalFreezeRetainArr,distanceNormalFreezeForgetArr,T_{converge\_i}Arr$
	\end{algorithmic}  
\end{algorithm}

% 如图\ref{fig:chapter4_process_2}所示。
% 我们首先使用正常训练集去训练神经网络，直至训练集准确率收敛，获得模型1。在训练网络之前保存神经网络训练的初始参数，记为模型0。
% 将模型1的最后一层的参数替换为模型0的最后一层参数，模型1其余层数的参数保持不变，由此得到模型2\_1。
% 将模型2\_1加载到一个新的神经网络中，使用保留训练集去训练这个新的网络，直至训练准确率收敛。在训练过程中所有参数都将会得到更新。训练收敛后得到模型3\_1。
% 然后再将模型1的后两层参数替换为模型0的后两层参数，得到模型2\_2。将模型2\_2用保留训练集去训练至模型收敛，在训练过程中所有参数都将会得到更新。模型收敛后得到模型3\_2。
% 以此类推，逐层重置并且不冻结参数，使用保留训练集训练至模型收敛。
% 最后再使用保留训练集完全重新训练一个神经网络，获得模型4。
% 将模型3\_i测量准确率、收敛时间和与模型4的激活距离，根据实验结果综合测算冻结参数的必要性。

% \subsubsection{反向冻结验证实验}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.25]{chapter4_process_3.png}
%     \caption{反向冻结验证实验流程图}
%     \label{fig:chapter4_process_3}
% \end{figure}

% 本实验是为了验证卷积神经网络的分层抽象特性，与正向冻结实验进行对照。
% 为了便于说明，我们称从输入端开始冻结网络的方式为正向冻结，称从输出端开始冻结网络的方式为反向冻结。
% 如图\ref{fig:chapter4_process_3}所示，
% 用正常训练集训练神经网络得到模型1，在正常训练集训练之前保留网络参数得到模型0。
% 用模型0的第一卷积层参数（我们称离输入层最近的卷积层为第一卷积层），替换模型1的第一卷积层参数，得到模型2\_1。
% 用神经网络加载模型2\_1，除了第一卷积层外，其余层数的参数全部冻结。用保留训练集训练至准确率收敛，得到模型3\_1。
% 然后再将模型0的第一层和第二层卷积参数替换掉模型1的相应层数的参数，得到模型2\_2。将模型2\_2除了前两层以外的全部参数冻结，使用保留训练集训练至模型收敛，得到模型3\_2。
% 以此类推，分别重置第一卷积层至最后一层全连接层参数，重复上述步骤至只有最后一层参数参与训练完成为止。在实验过程中记录下每个Epoch开始时间以及损失函数Loss值。

% 本实验中使用的遗忘测试集、保留训练集和保留测试集均和确定冻结层数实验中的相同。

\subsubsection{遗忘可持续性验证实验}
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{chapter4_continuous_forget_1.png}
    \caption{遗忘可持续性验证实验流程图}
    \label{fig:chapter4_continuous_forget_1}
\end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.25]{chapter4_process_4.png}
%     \caption{遗忘可持续性验证实验流程图}
%     \label{fig:chapter4_process_4}
% \end{figure}

本实验为了检验冻结重置方法随着遗忘类别数量的增多，其性能是否会发生明显变化。本实验在ResNet18网络上使用CIFAR10训练集完成的。
% 如图\ref{fig:chapter4_process_4}所示，
首先，将CIFAR10训练数据去除一个类别的数据，得到了保留训练集1；接着将原来完整的CIFAR10训练数据再去掉两个类别的数据便得到了保留训练集2。
以此类推，一共得到9个保留训练集。遗忘类别和保留类别的情况已在表\ref{tab:forget-continuous-kinds}中列出。
实验流程示意图如图\ref{fig:chapter4_continuous_forget_1}所示，准备完成保留训练集后，要准备两个网络模型。
一个是初始化参数的ResNet18模型，另外一个是使用CIFAR10训练完成的，并且将其靠近输出的6层参数初始化后的网络模型。
准备好了两个网络模型后，就使用保留训练集1到9分别训练两个模型，在完全初始化的网络模型上无需冻结参数，在后6层参数重置的网络模型上，未被重置参数的层数需要在训练过程中冻结参数。
训练完成后，完全初始化的网络将得到使用9个训练数据集训练出来的模型。同样，在后6层参数重置的网络上也能得到9个使用保留训练集训练出来的模型。
得到模型后，将模型进行各类指标的测试。到此实验结束。
其具体的实现如算法\ref{algorithm:continuousForget}所示。
\begin{algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{遗忘可持续性验证实验算法 continuousForget}
	\label{algorithm:continuousForget}
	\begin{algorithmic}[1]
        \REQUIRE 数据集所有类别$totalClasses$,网络收敛损失函数限值$T_{threshold}$
        \ENSURE  $T_{converge\_forget\_i}Arr$,$accRetrainRetainArr$,$accRetrainForgetArr$,\\$accForget\_i\_KindsRetainArr$,$accForget\_i\_KindsForgetArr$,\\$distanceForget\_i\_KindsRetainArr$,$distanceForget\_i\_KindsForgetArr$
        \STATE $T_{converge\_forget\_i}Arr$,$accRetrainRetainArr$,$accRetrainForgetArr$,\\$accForget\_i\_KindsRetainArr$,$accForget\_i\_KindsForgetArr$,\\$distanceForget\_i\_KindsRetainArr$,$distanceForget\_i\_KindsForgetArr$$ = []$
        \FOR {$i \to totalClasses.length - 1$}
            \STATE $C_{forget} \gets []$
            \FOR {$j \to i$}
                \STATE $C_{forget}.push(totalClasses[j])$
            \ENDFOR
            \STATE $D_{train\_retain},D_{train\_forget},D_{test\_retain},D_{test\_forget} \gets dispartDataset(trainset, testset, C_{forget})$(算法\ref{algorithm:dispartDataset})
            \STATE $retrainModel,T_{converge\_retrain} \gets trainNet(model\_init, trainRetain, [],$ \\ $ T_{threshold})$(算法\ref{algorithm:trainNet})
            \STATE $genModel, T_{converge\_forget\_i} \gets normalFreezeResetInnerCycle($\\$total\_layers, i, C_{forget},D_{retain\_train},D_{retain\_test}, D_{forget\_test},retrainModel )$(算法\ref{algorithm:normalFreezeResetInnerCycle})
            \STATE $accRetrainRetain \gets getAcc(retrainModel, D_{retain\_test})$(算法\ref{algorithm:getAcc})
            \STATE $accRetrainForget  \gets getAcc(retrainModel, D_{forget\_test})$(算法\ref{algorithm:getAcc})
            \STATE $accForget\_i\_KindsRetain \gets getAcc(genModel, D_{retain\_test})$(算法\ref{algorithm:getAcc})
            \STATE $accForget\_i\_KindsForget  \gets getAcc(genModel, D_{forget\_test})$(算法\ref{algorithm:getAcc})
            \STATE $distanceForget\_i\_KindsRetain \gets getDistance(genModel, modelRetrain,D_{retain\_test})$(算法\ref{algorithm:getDistance})
            \STATE $distanceForget\_i\_KindsForget \gets getDistance(genModel, modelRetrain, D_{forget\_test})$(算法\ref{algorithm:getDistance})
            \STATE $T_{converge\_forget\_i}Arr.push(T_{converge\_forget\_i})$
            \STATE $accRetrainRetainArr.push(accRetrainRetain)$
            \STATE $accRetrainForgetArr.push(accRetrainForget)$
            \STATE $accForget\_i\_KindsRetainArr.push(accForget\_i\_KindsRetain)$
            \STATE $accForget\_i\_KindsForgetArr.push(accForget\_i\_KindsForget)$
            \STATE $distanceForget\_i\_KindsRetainArr.push(distanceForget\_i\_KindsRetain)$
            \STATE $distanceForget\_i\_KindsForgetArr.push(distanceForget\_i\_KindsForget)$
        \ENDFOR
        \RETURN $T_{converge\_forget\_i}Arr$,$accRetrainRetainArr$,$accRetrainForgetArr$,\\$accForget\_i\_KindsRetainArr$,$accForget\_i\_KindsForgetArr$,\\$distanceForget\_i\_KindsRetainArr$,$distanceForget\_i\_KindsForgetArr$
	\end{algorithmic}  
\end{algorithm}
% 用正常训练集训练神经网络得到模型1，在正常训练集训练之前保留网络参数得到模型0。
% 用神经网络加载模型1，然后把神经网络全连接层和靠近输出层的5层卷积层的参数重置为模型0的相应层次的参数，得到模型2\_1到模型2\_9。
% 然后分别用保留集1-保留集9去训练模型，训练过程中冻结除了全连接层和后5层卷积层的参数。得到模型3\_1到模型3\_9。
% 然后使用保留集1-保留集9分别重新训练模型，得到模型4\_1到模型4\_9。
% 训练过程中记录各Epoch开始时间和损失函数Loss值。
% 与取得保留训练集类似，我们将测试集去掉一个类别的测试数据便得到保留测试集1，去掉2个类别的测试数据便得到保留测试集2。以此类推，一共得到9个保留测试集。
% 然后用9个保留测试集分别测试模型3\_1到模型3\_9，还有模型4\_1到模型4\_9的准确率、收敛时间以及激活距离。

\begin{table}
    \centering
    \caption{遗忘可持续性实验遗忘类别和保留类别}
    \begin{tabular}{cp{6cm}p{6cm}p{3cm}}
      \toprule
      遗忘类别数  & 遗忘类别 & 保留类别  \\
      \midrule
      1 & 卡车  & 飞机、汽车、鸟、猫、鹿、狗、青蛙、马、轮船  \\
      2 & 轮船、卡车  & 飞机、汽车、鸟、猫、鹿、狗、青蛙、马  \\
      3 & 马、轮船、卡车  & 飞机、汽车、鸟、猫、鹿、狗、青蛙  \\
      4 & 青蛙、马、轮船、卡车  & 飞机、汽车、鸟、猫、鹿、狗  \\
      5 & 狗、青蛙、马、轮船、卡车  & 飞机、汽车、鸟、猫、鹿  \\
      6 & 鹿、狗、青蛙、马、轮船、卡车  & 飞机、汽车、鸟、猫  \\
      7 & 猫、鹿、狗、青蛙、马、轮船、卡车  & 飞机、汽车、鸟  \\
      8 & 鸟、猫、鹿、狗、青蛙、马、轮船、卡车  & 飞机、汽车  \\
      9 & 汽车、鸟、猫、鹿、狗、青蛙、马、轮船、卡车  & 飞机  \\
      \bottomrule
    \end{tabular}
    \label{tab:forget-continuous-kinds}
\end{table}

\section{实验结果}
\subsection{确定冻结层数实验}
如图\ref{fig:chapter4_3}所示，图中展示了反向冻结参数训练后测试准确率的曲线。
绿色方块点划线和红色方块点线分别代表反向冻结参数时遗忘测试集和保留测试集的准确率数据。
为了进行对比，图中画了两条横线，x点线代表完全重新训练网络在保留测试集上的测试结果，*点线代表完全重新训练网络在遗忘测试集上的准确率数据。
从图中可以看出，经过反向冻结后，保留测试集的准确率和完全重新训练的准确率相近。然而在遗忘测试集上的准确率在重置参数的前几层是不为0的。
这个现象可以验证卷积神经网络具有分层抽象特性，而且靠近输入的网络参数与最终分类是基本无关的或者关联不大。
在前12层重置参数后利用保留训练集进行训练，遗忘测试集的准确率不为0。
这说明，重置前12层参数时并没有把遗忘类别很好地遗忘掉，有一些通过重置参数而丢失的信息可以通过保留训练集的学习补充回去。
在第13层以前，遗忘类别的分类器总能通过保留训练集学到的特征而产生一定的准确率。
直到参数重置到了第13层，遗忘类别的分类器无法通过保留训练集学习到的特征而对遗忘集进行准确分类。
我们能从这张图中找到“第一个零点”是第13层，于是完成了确定层数方法的第一步。
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_3.png}
    \caption{ResNet18-CIFAR10反向冻结重置参数测试准确率}
    \label{fig:chapter4_3}
\end{figure}

如图\ref{fig:chapter4_1}所示，图中展示了本文所讲述的重置冻结参数遗忘方法训练的网络用遗忘测试集和保留测试集测试之后得到准确率。
蓝色方块虚线代表利用保留测试集测试的准确率，红色圆点实线代表使用遗忘测试集测试的准确率。
从红色圆点实线可以看出，遗忘测试集的准确率全是0。这样的结果与完全重新训练得到的模型在遗忘测试集上的准确率完全相同。这说明使用本文提出的冻结重置参数方法，无论重置多少层参数，在遗忘集上均能达到理想的效果。
从蓝色方块虚线与上面的点线可以看出，本方法得到的模型在保留集测试准确率上在大部分情况下均好于完全重新训练得到的模型在保留集上的准确率。
从蓝色方块虚线中也发现随着重置参数的层数的增加，其保留集测试准确率有下降趋势，逐渐接近完全重新训练模型在保留集上的测试结果。
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_1.png}
    \caption{ResNet18-CIFAR10重置冻结参数遗忘方法测试准确率与重置参数层次的关系}
    \label{fig:chapter4_1}
\end{figure}

如图\ref{fig:chapter4_time_1}所示，图中展示了本文所讲述的重置冻结参数遗忘方法在重置不同层次参数上使用保留集训练后收敛时间的对比。
图中展示了三套柱图的对比，蓝色柱图、黄色柱图和绿色柱图分别代表使用保留训练集训练网络时，该训练批次（Epoch）的平均损失函数Loss的值首次降到0.1，0.05和0.03以下时的训练批次（Epoch）数。
批次数越小，说明训练的收敛时间越快。上面有三条不同线型的横线，点线、虚线和点划线分别代表完全重新训练网络时，该训练过程的平均损失函数值首次降到0.1，0.05和0.03以下时的训练批次（Epoch）数。
这三条横线的作用是与本文提到的方法进行收敛时间上的对比。
从图中的柱状图可以看出，随着重置参数的层数逐渐增多，其收敛的Epoch逐渐增大，逐渐接近完全重新训练时收敛的Epoch。
随着重置参数的层数增多，网络中需要更新的参数也逐渐增多，其状态也越来越接近完全重新训练的情况，所以收敛时间也逐步接近完全重新训练。
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{chapter4_time_1.png}
    \caption{ResNet18-CIFAR10冻结重置参数方法不同层数收敛时间对比}
    \label{fig:chapter4_time_1}
\end{figure}

如图\ref{fig:chapter4_distance_1}所示，图中展示了本文所讲述的重置冻结参数遗忘方法训练的网络与完全重新训练的网络之间对于测试集输出之间的激活距离。
激活距离在上一节有具体讲到，是两个网络输出向量差的绝对值的第二范数值在测试数据集上的期望。
蓝色实线代表两个网络在遗忘测试集上的激活距离，黄色虚线代表两个网络在保留集上的激活距离。从图中可以看出，两个网络在遗忘测试集上的激活距离要高于在保留测试集上的激活距离。
我们对于两个网络激活距离的期望是越小越好，在前7层可以看到保留测试集的激活距离普遍稳定地保持较低数值，而对于遗忘测试集的激活距离前2层的数值相对较大，3、4、5层数值也略大，从第6层开始，数值变得平稳。
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_distance_1.png}
    \caption{ResNet18-CIFAR10重置冻结参数遗忘后与完全重新训练网络的激活距离}
    \label{fig:chapter4_distance_1}
\end{figure}

我们根据上面三个指标的测试结果确定分层，这也是确定分层方法的第二步。
预选层次是靠近输入的第13层，我们首先在图\ref{fig:chapter4_1}中找到靠近输入的第13层，就是靠近输出的第6层，因为我们使用的网络一共是18层。因此在图\ref{fig:chapter4_1}中应当看第6层。
可以看到，第6层的遗忘测试准确率和完全重新训练相同，保留测试准确率也与完全重新训练相差不大， 而且还略高于完全重新训练的准确率。
其次，在图\ref{fig:chapter4_time_1}中可以看到，第6层的Loss为0.05的收敛时间是9，而完全重新训练Loss为0.05的收敛时间是20。与完全重新训练相比，收敛时间能缩短一倍还多。
最后看激活距离，在图\ref{fig:chapter4_distance_1}中可以看到，第6层的激活距离无论是遗忘类别的激活距离还是保留类别的激活距离，均接近相当于完全重新训练的第18层的激活距离。
综合考虑预选层数和各个指标在该层数上的表现，我们最终选择靠近输出层的第6层作为本文方法的重置参数的层数，也就是靠近输出的后6层需要重置。
到此，我们完成了ResNet18神经网络的分层步骤。

我们也在改造后的VGG16网络结构上利用CIFAR10数据集进行了确定层数实验。
如图\ref{fig:chapter4_vgg_acc_2}所示，图中展示了使用14层的VGG16网络和CIFAR10数据进行反向冻结实验的测试准确率情况。
从红色圆点实线中可以看出，在重置参数层数为1到5时，网络收敛后遗忘测试集仍然有较高的准确率。到第6层以后，遗忘测试集准确率才达到了和完全重新训练相近的情况。
因此我们找到的“第一个零点”是靠近输入的第6层。
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_vgg_acc_2.png}
    \caption{VGG16-CIFAR10反向冻结重置参数测试准确率}
    \label{fig:chapter4_vgg_acc_2}
\end{figure}

下面是正向冻结实验结果以评估预选层次。
实验结果如图\ref{fig:chapter4_vgg_acc_1}所示，我们先找到图中靠近输入的第6层在该图中的层数。因为我们的网络是14层，因此在图\ref{fig:chapter4_vgg_acc_1}中，该层次对应的是靠近输出的第9层。
红色圆点实线代表使用本文方法在遗忘测试集上得到的准确率。可以看出在第9层，遗忘测试集准确率和完全重新训练相同。
蓝色圆点虚线表示使用本文方法在保留测试集上得到的准确率，可以看出在第9层，保留测试集准确率和完全重新训练保持接近，并且还略高于完全重新训练的准确率。

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_vgg_acc_1.png}
    \caption{VGG16-CIFAR10重置冻结参数遗忘方法测试准确率与重置参数层次的关系}
    \label{fig:chapter4_vgg_acc_1}
\end{figure}

在图\ref{fig:chapter4_vgg_time_1}中可以看到，第9层Loss=0.05的收敛时间是12，而完全重新训练Loss=0.05的收敛时间是32，收敛时间缩短了一倍还多。
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{chapter4_vgg_time_1.png}
    \caption{VGG16-CIFAR10冻结重置参数方法不同层数收敛时间对比}
    \label{fig:chapter4_vgg_time_1}
\end{figure}

在VGG16网络结构上的激活距离测试结果如图\ref{fig:chapter4_vgg_distance_1}所示，蓝色圆点实线代表本文方法在VGG16结构上使用遗忘集测试的激活距离，发现第9层以后激活距离处于较低且平稳的数值。
黄色圆点虚线代表使用本文方法在VGG16结构上使用保留集测试的激活距离，可以发现无论重置参数层数如何，激活距离均保持较低且平稳的数值。综合以上的准确率、收敛时间和激活距离，我们选择从输出层开始第9层作为本文方法重置参数的层次。
因为重置该层参数后保留集和遗忘集准确率均与完全重新训练保持相近，在激活距离上处于较低数值，而且在收敛时间上也不是很长。
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_vgg_distance_1.png}
    \caption{VGG16-CIFAR10重置冻结参数遗忘后与完全重新训练网络的激活距离}
    \label{fig:chapter4_vgg_distance_1}
\end{figure}

\subsection{冻结必要性验证实验}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_2.png}
    \caption{不冻结参数与冻结参数准确率的对比}
    \label{fig:chapter4_2}
\end{figure}

如图\ref{fig:chapter4_2}所示，图中展示了重置一定参数后冻结参数与不冻结参数训练对遗忘类别和保留类别准确率的影响。
黄色圆点虚线代表冻结参数情况下网络在保留测试集上的准确率，蓝色圆点实线代表该网络在遗忘测试集上得到的准确率。
红色方块点线代表不冻结参数的情况下在保留测试集上的准确率，绿色方格点划线代表该网络在遗忘测试集上得到的准确率。
除此之外，图中还画了两条横线，x点线代表完全重新训练的网络模型在保留测试集上得到的准确率，星点线代表完全重新训练的网络模型在遗忘测试集上得到的准确率。
这两条横线的主要作用是作为冻结参数和不冻结参数准确率的参照。通过黄色圆点虚线和红色方块点线可以看出冻结参数与不冻结参数在保留测试集的准确率上总体相差不大，但是冻结参数的方法要略好于不冻结参数方法。
在遗忘测试集的准确率上，两种方法效果相同，均为0。
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{chapter4_time_2.png}
    \caption{冻结参数与不冻结参数收敛时间对比}
    \label{fig:chapter4_time_2}
\end{figure}

如图\ref{fig:chapter4_time_2}所示，图中展示了冻结参数与不冻结参数重置参数后使用保留训练集训练网络的收敛时间。
蓝色、黄色和绿色柱图分别代表冻结参数情况下网络的平均损失函数值收敛到0.1、0.05和0.03时所花费的训练周期，即Epoch数。
粉色、紫色和棕色柱图分别代表非冻结参数情况下网络的平均损失函数收敛到0.1、0.05和0.03时所花费的训练周期数。
上面画出的三条横线，点线、虚线和点划线分别代表完全重新训练网络过程中，网络的平均损失函数值收敛至0.1、0.05和0.03时所花费的训练周期数。其主要作用是用来提供参照。
通过观察冻结参数和不冻结参数的柱状图可以发现，冻结参数后训练的收敛时间从整体上要少于不冻结参数训练网络收敛时间。这个现象也是符合逻辑的。
冻结参数后，一部分参数不需要计算更新，而没有冻结参数的网络则需要计算所有参数的更新。所以从计算量上分析，不冻结参数训练网络所需要的工作量是比冻结参数要大的。
通过图\ref{fig:chapter4_2}和图\ref{fig:chapter4_time_2}的结果可以初步得出结论，冻结参数训练网络的方法要好于不冻结参数训练网络的方法。
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_distance_2.png}
    \caption{冻结参数与不冻结参数激活距离对比}
    \label{fig:chapter4_distance_2}
\end{figure}

如图\ref{fig:chapter4_distance_2}所示，图中展示了冻结参数与不冻结参数的两个网络在激活距离上的对比。这两个网络参照的对象均是完全重新训练生成的网络。
蓝色圆点实线和黄色圆点虚线分别代表冻结参数的情况下，分别使用遗忘测试集和保留测试集测试网络，两个网络的输出与重新训练网络的输出距离。
同样，绿色方块点线和红色方块点划线分别代表在不冻结参数情况下分别用遗忘测试集和保留测试集进行测试，两个网络的输出与重新训练网络输出的距离。
从图中可以看出，冻结参数的方法和不冻结参数的方法，在保留测试集上的激活距离相差无几，并且从整体水平上低于在遗忘测试集上产生的激活距离。
在用遗忘测试集测试的网络输出距离上，冻结参数的方法在前三层时激活距离比不冻结参数的要大，可是从第三层往后的层次冻结与不冻结的激活距离很相近。因此重置参数的层数选择第三层以后，这个差异是可以忽略不计的。

% \subsection{反向冻结验证实验}

\subsection{遗忘可持续性验证实验}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_4.png}
    \caption{遗忘可持续性实验测试准确率数据}
    \label{fig:chapter4_4}
\end{figure}

如图\ref{fig:chapter4_4}所示，图中展示了遗忘可持续性实验测试准确率的结果。
图中的横轴代表遗忘类别的数量，本文使用的数据集是CIFAR10数据集，一共有10个类别，因此遗忘类别的最大数量是9个。图中纵轴则是测试的准确率。
图中蓝色圆点实线和黄色圆点虚线分别代表在冻结重置参数方法下在遗忘测试集和保留测试集上的准确率数据。绿色方块点划线和红色方块点线分别代表完全重新训练后，在遗忘测试集和保留测试集上得到的准确率数据。
从图中可以看出无论遗忘类别如何变化，遗忘测试集的准确率始终为0，保留集的准确率始终处于较高水平。和完全重新训练相比，冻结重置参数方法在保留集的测试准确率上还略高于完全重新训练的准确率。
这说明无论遗忘掉多少类别，冻结重置参数的方法在测试准确率上均能取得很好效果。

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{chapter4_time_3.png}
    \caption{遗忘可持续性实验收敛时间对比}
    \label{fig:chapter4_time_3}
\end{figure}

如图\ref{fig:chapter4_time_3}所示，图中展示了遗忘可持续性实验不同遗忘类别数下收敛时间的对比情况。
图中横轴是遗忘类别数，纵轴是收敛时间。
图中蓝色、黄色和绿色柱图代表冻结重置参数方法在本文遗忘方法下训练收敛的批次数。红色、紫色和棕色柱图分别代表完全重新训练的收敛批次数。
从图中可以看出，除了遗忘9个类别以外，其余遗忘类别数冻结重置参数方法的收敛时间从整体上要低于完全重新训练的收敛时间。因此无论需要遗忘多少类别，冻结重置参数方法在时间上与完全重新训练相比是节省时间的。

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{chapter4_distance_4.png}
    \caption{遗忘可持续性实验激活距离对比}
    \label{fig:chapter4_distance_4}
\end{figure}

如图\ref{fig:chapter4_distance_4}所示，图中展示了遗忘可持续性实验随着遗忘类别的增加激活距离的对比情况。
图中蓝色实线代表经过冻结重置方法遗忘过后的网络，在遗忘测试集上与完全重新训练网络的输出距离。黄色虚线则代表两个网络在保留集上的激活距离。
从图中可以看出，无论是遗忘测试集还是保留测试集，激活距离在整体上是处于较小的数值，这就说明无论遗忘多少类别，本文的遗忘方法与完全重新训练的模型在激活距离上保持得很近。

\section{本章小结}
本章对第三章提出的冻结重置参数的遗忘方法进行了实验验证。首先我们设计了确定重置参数层次的实验。确定的准则通过预选层数和三个指标来综合考量。
其次，我们设计了冻结和不冻结参数的对比试验，目的是验证冻结参数是有必要的。结果很好地支持了本文提出的方法。
通过反向重置参数的遗忘准确率曲线可以看出，反向重置参数并不能使得遗忘类别达到很好的遗忘效果。而正向重置参数却可以达到很好的遗忘效果，从而验证卷积神经网络的分层抽象特性。
最后为了验证遗忘方法的遗忘可持续性，我们设计了遗忘可持续性实验。通过在准确率、激活距离和收敛时间上面的观察，可以发现无论遗忘类别的数量有多少，冻结重置参数的方法均能达到很好的效果。

